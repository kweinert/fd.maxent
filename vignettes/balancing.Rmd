---
title: "Example: Survey Reweighting"
author: "Karsten W."
date: "2022-12-10"
output: html_document
vignette: >
  %\VignetteIndexEntry{balancing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: "literature.bib"
---

## The Problem

This is a problem from my day job. Hence I cannot show the real data here and use synthetic data.

This year I was working in a team doing a program evaluation. So there is grants data and we conduct a survey to collect data that is not in the grant database. As it happens, only 30% or so answer the survey and there is a distortion in the survey data with respect to some structural variables. For instance, SME tend to answer less often than large enterprises.

Now one idea to deal with this problem is to reweigh / post-stratify the survey data so that the rebalanced survey dataset has the same moments as the grant data. What I try in this example is to do this using the maximum entropy principle. I don't use this at work (yet).

The idea stems from a paper and R package `ebal` by [Jens Hainmueller](https://doi.org/10.1093/pan/mpr025). The package `WeightIt` seems to have implemented a similar method using `optim`. Also the famous `survey` package seems to have functions (`postStratify`, `calibrate`) that achieve what I am trying to do here.

An interesting read on this topic is [this](https://www.surveypractice.org/article/2809-post-stratification-or-non-response-adjustment).

## Required Packages

In addition to `fd.maxent`, we need the `survey` package for comparing our results with a serious implementation.

```{r pkg, echo=TRUE}
library(fd.maxent, quietly=TRUE)
library(survey, quietly=TRUE)
```

## The Data

Let's build some fake grant data. As structural variables we consider the size of the
grantee, the sector, and the project type. As survey response rate we assume 26%. We
simulate a lower response rate from SME. To make things more interesting, we also add an interaction between `sector` and `size`, assuming that in "sector A" there are only few SME (think steel industry or oil refineries).

```{r data, echo=TRUE}
grants <- data.frame(
	id=paste0("id",1:1000),
	size=c(rep("SME", 700), rep("LE", 300))
) |> transform(
	sector=ifelse(size=="SME", 
		sample(paste("Sector", LETTERS[1:4]), 1000, replace=TRUE, prob=c(0.1, 0.3, 0.3, 0.3)),
		sample(paste("Sector", LETTERS[1:4]), 1000, replace=TRUE)
	),
	prj_type=sample(paste("Type", 1:3), 1000, replace=1000)
)
svy <- data.frame(
	id=paste0("id", sample(1:1000, 260, prob=c(rep(0.2, 700), rep(0.4, 300)))),
	question=sample(c("yes", "maybe", "no", NA), 260, replace=TRUE, prob=c(0.3, 0.3, 0.3, 0.1))
) |> merge(grants, by="id", all.x=TRUE)
```

The synthetic data exhibit the mentioned distortion by design:

```{r distort, echo=TRUE}
table(grants$size) |> proportions()
table(svy$size) |> proportions()
```

## Setting up the Problem

We have 260 variables to determine, one for each survey response. The number of constraints is calculated beforehand, see below how they get set.

```{r setup}
# set up mep
mep <- mep_make(
	nvar=260, 
	ncons=1+3+2+1, 
	control=list(method="BB")
)
```

## Adding Constraints

For each of the structural variables `size`, `sector`, and `prj_type`, we calculate the
proportions for the levels and create a constraint for each but the last level.

```{r cons1}
# size
j <- 1
props <- table(grants[,"size"]) |> proportions()
idx <- which(svy[,"size"]=="SME")
mep <- mep_set_constraint(mep, j=j, xt=1, indices=idx, rhs=props["SME"])
j <- j + 1

# sector
props <- table(grants[,"sector"]) |> proportions()
for (x in head(unique(grants[,"sector"]), -1)) {
	idx <- which(svy[,"sector"]==x)
	mep <- mep_set_constraint(mep, j=j, xt=1, indices=idx, rhs=props[x])
	j <- j + 1
}

# prj_type
props <- table(grants[,"prj_type"]) |> proportions()
for (x in head(unique(grants[,"prj_type"]), -1)) {
	idx <- which(svy[,"prj_type"]==x)
	mep <- mep_set_constraint(mep, j=j, xt=1, indices=idx, rhs=props[x])
	j <- j + 1
}
```

The last constraint is the usual normalization constraint. 

```{r cons2}
# must sum up to 1
mep <- mep_set_constraint(mep, j=j, xt=rep(1, 260), indices=1:260, rhs=1)
```

## Solve and Check the Result

Let's run the solver and see what we've got.

```{r solve}
mep <- mep_solve(mep, verbose=FALSE)
solution <- mep_getvars(mep)
mep_dispose(mep)
svy <- transform(svy, weight=solution*260)
```

Let's compare the `size` proportions in the weighted survey to `grants`:

```{r check}
# size
aggregate(weight~size, svy, function(x) sum(x)/260)
table(grants[,"size"]) |> proportions()
```

This looks as expected. The same is true for `sector` and `prj_type` (not shown here). 

How much vary the weights? 

```{r hist}
summary(svy$weight)
hist(svy$weight, breaks=30, xlab="Weight", main="")
```

The weights are not as evenly distributed as I expected.

All in all, the method seems to work. In some simulations some cases get weighted by a factor of 1.8, which seems quite high to me. 

We could also look at the 2-combinations, e.g. "SME in sector A". But I suppose this would increase the variance of the weights even more. 

## All in one step: `balance`

This vignette has shown step by step how to rebalance a dataset using the maximum entropy principle. For convenience, this approach has been wrapped into a function `balance`. So instead of following the steps in this vignette, we could also do:

```{r fun}
svy <- transform(svy, weight=balance(svy, grants, c("size", "sector", "prj_type")))
```

Use at your own risk. Especially for small `nrow(svy)` and/or larger distortions this method might fail without clearly indicating it.

Let's compare it with `survey::rake`. I followed this [helpful blogpost](https://tophcito.blogspot.com/2014/04/survey-computing-your-own-post.html) and came up with:

```{r survey}
library(survey, quietly=TRUE)
svy_d <- survey::svydesign(ids=~1, data=svy)
size_dist <- table(grants[,"size"]) |> 
	proportions() |> as.data.frame() |>
	transform(Freq=Freq*nrow(svy))
colnames(size_dist)[1] <- "size"
sector_dist <- table(grants[,"sector"]) |> 
	proportions() |> as.data.frame() |>
	transform(Freq=Freq*nrow(svy))
colnames(sector_dist)[1] <- "sector"
prj_type_dist <- table(grants[,"prj_type"]) |> 
	proportions() |> as.data.frame() |>
	transform(Freq=Freq*nrow(svy))
colnames(prj_type_dist)[1] <- "prj_type"
svy_d <- survey::rake(design=svy_d,
   sample.margins = list(~size, ~sector, ~prj_type),
   population.margins = list(size_dist, sector_dist, prj_type_dist)
)
plot(weights(svy_d), svy[,"weight"], ylab="MaxEnt", xlab="survey::rake")
```

Looks like both methods produce the same result. This is very re-assuring.

The famous `survey` package also offers `survey::calibrate` function, which allows us to incorporate information on interactions (e.g. the `~size+sector` interaction) of the structural variables. The function has a `bound` parameter (but fails to converge if I set it), and a `trim` parameter that works perfectly.

In that case, the comparison of the weights shows some differences.

```{r calib}
svy_d <- survey::svydesign(ids=~1, data=svy)
size_sector <- xtabs(~size+sector, grants) |> proportions()
size_sector <- size_sector*260
size_prj_type <- xtabs(~size+prj_type, grants) |> proportions()
size_prj_type <- size_prj_type*260
sector_prj_type <- xtabs(~sector+prj_type, grants) |> proportions()
sector_prj_type <- sector_prj_type*260
svy_d <- survey::calibrate(
	svy_d, list(~size+sector, ~size+prj_type, ~sector+prj_type), 
    population=list(size_sector, size_prj_type, sector_prj_type),
	trim=c(0.3,3),
	calfun="raking"
)
plot(weights(svy_d), svy[,"weight"], ylab="MaxEnt", xlab="survey::calibrate(xtabs,trim)")
```

It should be possible to include the interactions as constraints in the maximum entropy formulation, but it requires still some work to include inequalities as constraints. So I end this example here, having learnt some directions I want to go with the maximum entropy solver (inequality constraints / slack variables) and having learnt to use some functions from the `survey` package, which seems definitely the way to go.

